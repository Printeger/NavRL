"""
è®­ç»ƒè„šæœ¬ (Training Script)
===========================
è¿™æ˜¯æ•´ä¸ªé¡¹ç›®çš„ä¸»å…¥å£ï¼Œè´Ÿè´£è®­ç»ƒæ— äººæœºå¯¼èˆªçš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹ã€‚

ä¸»è¦æµç¨‹ï¼š
1. å¯åŠ¨ Isaac Sim ä»¿çœŸå™¨
2. åˆå§‹åŒ– WandB æ—¥å¿—è®°å½•
3. åˆ›å»ºè®­ç»ƒç¯å¢ƒï¼ˆåœ°å½¢ã€éšœç¢ç‰©ã€ä¼ æ„Ÿå™¨ï¼‰
4. åˆ›å»º PPO ç­–ç•¥ç½‘ç»œ
5. æ”¶é›†äº¤äº’æ•°æ®å¹¶è®­ç»ƒæ¨¡å‹
6. å‘¨æœŸæ€§è¯„ä¼°å’Œä¿å­˜æ¨¡å‹

ä½œè€…ï¼šNavRL é¡¹ç›®
"""

import argparse
import os
import hydra              # é…ç½®ç®¡ç†æ¡†æ¶
import datetime
import wandb              # å®éªŒè·Ÿè¸ªå·¥å…·
import torch
from omegaconf import DictConfig, OmegaConf
from omni.isaac.kit import SimulationApp     # Isaac Sim åº”ç”¨
from ppo import PPO                          # PPO ç®—æ³•å®ç°
from omni_drones.controllers import LeePositionController  # ä½å±‚æ§åˆ¶å™¨
from omni_drones.utils.torchrl.transforms import VelController, ravel_composite
from omni_drones.utils.torchrl import SyncDataCollector, EpisodeStats  # æ•°æ®æ”¶é›†å™¨
from torchrl.envs.transforms import TransformedEnv, Compose
from utils import evaluate  # è¯„ä¼°å‡½æ•°
from torchrl.envs.utils import ExplorationType


# ============================================
# é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆtrain.yaml ç­‰ï¼‰
# ============================================
FILE_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), "cfg")

@hydra.main(config_path=FILE_PATH, config_name="train", version_base=None)
def main(cfg):
    """
    ä¸»è®­ç»ƒå‡½æ•°
    
    å‚æ•°:
        cfg: Hydra é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰è®­ç»ƒå‚æ•°
             - cfg.headless: æ˜¯å¦æ— å¤´æ¨¡å¼ï¼ˆä¸æ˜¾ç¤ºGUIï¼‰
             - cfg.env.num_envs: å¹¶è¡Œç¯å¢ƒæ•°é‡
             - cfg.max_frame_num: æ€»è®­ç»ƒå¸§æ•°
             - cfg.algo: PPO ç®—æ³•è¶…å‚æ•°
             - cfg.sensor: ä¼ æ„Ÿå™¨é…ç½®ï¼ˆLiDARï¼‰
    """
    # ============================================
    # ç¬¬ 1 æ­¥ï¼šå¯åŠ¨ Isaac Sim ä»¿çœŸå™¨
    # ============================================
    # SimulationApp æ˜¯ Isaac Sim çš„æ ¸å¿ƒï¼Œè´Ÿè´£ï¼š
    #   - åˆ›å»º 3D ä»¿çœŸåœºæ™¯
    #   - è¿è¡Œç‰©ç†å¼•æ“ï¼ˆPhysXï¼‰
    #   - æ¸²æŸ“å›¾å½¢ï¼ˆå¦‚æœ headless=Falseï¼‰
    sim_app = SimulationApp({"headless": cfg.headless, "anti_aliasing": 1})

    # ============================================
    # ç¬¬ 2 æ­¥ï¼šåˆå§‹åŒ– WandB å®éªŒè·Ÿè¸ª
    # ============================================
    # WandB ç”¨äºè®°å½•å’Œå¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ï¼š
    #   - æŸå¤±æ›²çº¿ï¼ˆactor_loss, critic_lossï¼‰
    #   - è®­ç»ƒæŒ‡æ ‡ï¼ˆæˆåŠŸç‡ã€ç¢°æ’ç‡ã€å›æŠ¥ï¼‰
    #   - è§†é¢‘å½•åˆ¶ï¼ˆè¯„ä¼°æ—¶çš„æ— äººæœºé£è¡Œï¼‰
    
    # å°† Hydra çš„ DictConfig è½¬æ¢ä¸ºæ™®é€šå­—å…¸ï¼Œé¿å…åºåˆ—åŒ–é”™è¯¯
    wandb_config = OmegaConf.to_container(cfg, resolve=True)
    
    if (cfg.wandb.run_id is None):
        # æ–°å»ºä¸€ä¸ªè®­ç»ƒè¿è¡Œï¼ˆrunï¼‰
        run = wandb.init(
            project=cfg.wandb.project,  # WandB é¡¹ç›®åç§°
            name=f"{cfg.wandb.name}/{datetime.datetime.now().strftime('%m-%d_%H-%M')}",
            entity=cfg.wandb.entity,    # WandB ç”¨æˆ·å/å›¢é˜Ÿå
            config=wandb_config,        # ä¿å­˜æ‰€æœ‰é…ç½®å‚æ•°
            mode=cfg.wandb.mode,        # "offline" æˆ– "online"
            id=wandb.util.generate_id(),
        )
    else:
        # æ¢å¤ä¹‹å‰ä¸­æ–­çš„è®­ç»ƒè¿è¡Œ
        run = wandb.init(
            project=cfg.wandb.project,
            name=f"{cfg.wandb.name}/{datetime.datetime.now().strftime('%m-%d_%H-%M')}",
            entity=cfg.wandb.entity,
            config=wandb_config,
            mode=cfg.wandb.mode,
            id=cfg.wandb.run_id,
            resume="must"  # å¿…é¡»æ¢å¤ä¹‹å‰çš„è¿è¡Œ
        )

    # ============================================
    # ç¬¬ 3 æ­¥ï¼šåˆ›å»ºå¯¼èˆªè®­ç»ƒç¯å¢ƒ
    # ============================================
    # NavigationEnv åŒ…å«æ‰€æœ‰ä»¿çœŸå…ƒç´ ï¼š
    #   - æ— äººæœºæ¨¡å‹ï¼ˆHummingbird å››æ—‹ç¿¼ï¼‰
    #   - LiDAR ä¼ æ„Ÿå™¨ï¼ˆ36Ã—4=144 ä¸ªæµ‹é‡ç‚¹ï¼‰
    #   - é™æ€éšœç¢ç‰©ï¼ˆåœ°å½¢ä¸Šçš„éšœç¢ç‰©ï¼‰
    #   - åŠ¨æ€éšœç¢ç‰©ï¼ˆç§»åŠ¨çš„ç«‹æ–¹ä½“å’Œåœ†æŸ±ï¼‰
    #   - å¥–åŠ±å‡½æ•°å’Œç»ˆæ­¢æ¡ä»¶
    from env import NavigationEnv
    env = NavigationEnv(cfg)

    # ============================================
    # ç¬¬ 4 æ­¥ï¼šåŒ…è£…ç¯å¢ƒï¼ˆæ·»åŠ æ§åˆ¶å™¨ï¼‰
    # ============================================
    # ä¸ºä»€ä¹ˆéœ€è¦åŒ…è£…ï¼Ÿ
    # - ç­–ç•¥ç½‘ç»œè¾“å‡ºï¼šé€Ÿåº¦æŒ‡ä»¤ï¼ˆvx, vy, vzï¼‰
    # - æ— äººæœºéœ€è¦ï¼šç”µæœºæ¨åŠ›ï¼ˆ4 ä¸ªç”µæœºçš„è½¬é€Ÿï¼‰
    # - VelControllerï¼šå°†é€Ÿåº¦æŒ‡ä»¤è½¬æ¢ä¸ºç”µæœºæ¨åŠ›
    transforms = []
    
    # Lee Position Controller: ç»å…¸çš„å››æ—‹ç¿¼æ§åˆ¶ç®—æ³•
    # å‚æ•°ï¼šé‡åŠ›åŠ é€Ÿåº¦ 9.81 m/sÂ², æ— äººæœºç‰©ç†å‚æ•°
    controller = LeePositionController(9.81, env.drone.params).to(cfg.device)
    vel_transform = VelController(controller, yaw_control=False)  # ä¸æ§åˆ¶åèˆªè§’
    transforms.append(vel_transform)
    
    # åº”ç”¨å˜æ¢å¹¶è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    transformed_env = TransformedEnv(env, Compose(*transforms)).train()
    transformed_env.set_seed(cfg.seed)    
    
    # ============================================
    # ç¬¬ 5 æ­¥ï¼šåˆ›å»º PPO ç­–ç•¥ç½‘ç»œ
    # ============================================
    # PPOï¼ˆProximal Policy Optimizationï¼‰åŒ…å«ï¼š
    #   1. Feature Extractor: CNN å¤„ç† LiDAR æ•°æ®
    #   2. Actorï¼ˆç­–ç•¥ç½‘ç»œï¼‰: è¾“å‡ºåŠ¨ä½œåˆ†å¸ƒ
    #   3. Criticï¼ˆä»·å€¼ç½‘ç»œï¼‰: è¯„ä¼°çŠ¶æ€ä»·å€¼
    policy = PPO(cfg.algo, transformed_env.observation_spec, transformed_env.action_spec, cfg.device)

    # ============================================
    # ç¬¬ 6 æ­¥ï¼šï¼ˆå¯é€‰ï¼‰åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    # ============================================
    # å¦‚æœæƒ³ç»§ç»­ä¹‹å‰çš„è®­ç»ƒï¼Œå¯ä»¥å–æ¶ˆæ³¨é‡Šä»¥ä¸‹ä»£ç ï¼š
    # checkpoint = "/path/to/checkpoint.pt"
    # policy.load_state_dict(torch.load(checkpoint))
    
    # ============================================
    # ç¬¬ 7 æ­¥ï¼šåˆ›å»ºç»Ÿè®¡æ•°æ®æ”¶é›†å™¨
    # ============================================
    # EpisodeStats ç”¨äºè·Ÿè¸ªæ¯ä¸ª episode çš„ç»Ÿè®¡ä¿¡æ¯ï¼š
    #   - return: ç´¯ç§¯å¥–åŠ±
    #   - reach_goal: æ˜¯å¦åˆ°è¾¾ç›®æ ‡
    #   - collision: æ˜¯å¦å‘ç”Ÿç¢°æ’
    #   - episode_len: episode é•¿åº¦
    episode_stats_keys = [
        k for k in transformed_env.observation_spec.keys(True, True) 
        if isinstance(k, tuple) and k[0]=="stats"
    ]
    episode_stats = EpisodeStats(episode_stats_keys)

    # ============================================
    # ç¬¬ 8 æ­¥ï¼šåˆ›å»ºå¼ºåŒ–å­¦ä¹ æ•°æ®æ”¶é›†å™¨
    # ============================================
    # SyncDataCollector è´Ÿè´£ï¼š
    #   1. è®©ç­–ç•¥ä¸ç¯å¢ƒäº¤äº’ï¼Œæ”¶é›†ç»éªŒæ•°æ®
    #   2. æ¯æ¬¡æ”¶é›† frames_per_batch å¸§æ•°æ®
    #   3. è‡ªåŠ¨é‡ç½®å®Œæˆçš„ç¯å¢ƒ
    collector = SyncDataCollector(
        transformed_env,
        policy=policy, 
        frames_per_batch=cfg.env.num_envs * cfg.algo.training_frame_num,  # æ¯æ‰¹æ•°æ®é‡
        total_frames=cfg.max_frame_num,      # æ€»è®­ç»ƒå¸§æ•°ï¼ˆè®­ç»ƒåœæ­¢æ¡ä»¶ï¼‰
        device=cfg.device,
        return_same_td=True,  # åŸåœ°æ›´æ–°ï¼ŒèŠ‚çœå†…å­˜
        exploration_type=ExplorationType.RANDOM,  # è®­ç»ƒæ—¶ä½¿ç”¨éšæœºæ¢ç´¢
    )

    # ============================================
    # ç¬¬ 9 æ­¥ï¼šä¸»è®­ç»ƒå¾ªç¯ ğŸ”„
    # ============================================
    # collector æ˜¯ä¸€ä¸ªè¿­ä»£å™¨ï¼Œæ¯æ¬¡è¿­ä»£ï¼š
    #   1. ä¸ç¯å¢ƒäº¤äº’æ”¶é›† frames_per_batch å¸§æ•°æ®
    #   2. è¿”å›ä¸€ä¸ª TensorDictï¼ŒåŒ…å« (state, action, reward, next_state)
    for i, data in enumerate(collector):
        # data çš„ç»“æ„ï¼š
        # {
        #   "agents": {
        #     "observation": {"lidar": [...], "state": [...], ...},
        #     "action": [...],
        #     "reward": [...]
        #   },
        #   "next": {...},  # ä¸‹ä¸€ä¸ªçŠ¶æ€
        #   "done": [...],
        #   "terminated": [...]
        # }
        
        # -------- è®°å½•åŸºæœ¬ä¿¡æ¯ --------
        info = {
            "env_frames": collector._frames,  # å·²è®­ç»ƒçš„æ€»å¸§æ•°
            "rollout_fps": collector._fps      # æ•°æ®æ”¶é›†é€Ÿåº¦ï¼ˆå¸§/ç§’ï¼‰
        }

        # -------- è®­ç»ƒç­–ç•¥ç½‘ç»œ --------
        # policy.train() æ‰§è¡Œï¼š
        #   1. è®¡ç®— GAE ä¼˜åŠ¿å‡½æ•°
        #   2. è¿›è¡Œå¤šè½®ï¼ˆepochsï¼‰å°æ‰¹é‡ï¼ˆminibatchï¼‰æ›´æ–°
        #   3. è¿”å›æŸå¤±ç»Ÿè®¡ä¿¡æ¯
        train_loss_stats = policy.train(data)
        info.update(train_loss_stats)  # æ·»åŠ è®­ç»ƒæŸå¤±ä¿¡æ¯

        # -------- ç»Ÿè®¡è®­ç»ƒ episode ä¿¡æ¯ --------
        episode_stats.add(data)
        if len(episode_stats) >= transformed_env.num_envs:
            # æ‰€æœ‰ç¯å¢ƒéƒ½è‡³å°‘å®Œæˆä¸€ä¸ª episodeï¼Œè®¡ç®—å¹³å‡ç»Ÿè®¡
            stats = {
                "train/" + (".".join(k) if isinstance(k, tuple) else k): torch.mean(v.float()).item() 
                for k, v in episode_stats.pop().items(True, True)
            }
            info.update(stats)

        # -------- å‘¨æœŸæ€§è¯„ä¼°ç­–ç•¥ --------
        if i % cfg.eval_interval == 0:
            print("[NavRL]: start evaluating policy at training step: ", i)
            
            # å¼€å¯æ¸²æŸ“ï¼ˆç”¨äºå½•åˆ¶è§†é¢‘ï¼‰
            env.enable_render(True)
            env.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            
            # è¿è¡Œè¯„ä¼°ï¼šä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼ˆMEANï¼‰ï¼Œä¸éšæœºæ¢ç´¢
            eval_info = evaluate(
                env=transformed_env, 
                policy=policy,
                seed=cfg.seed, 
                cfg=cfg,
                exploration_type=ExplorationType.MEAN  # ç¡®å®šæ€§åŠ¨ä½œ
            )
            
            # æ¢å¤åŸæ¥çš„æ¸²æŸ“è®¾ç½®
            env.enable_render(not cfg.headless)
            env.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼
            env.reset()
            info.update(eval_info)
            print("\n[NavRL]: evaluation done.")
        
        # -------- è®°å½•åˆ° WandB --------
        run.log(info)

        # -------- å‘¨æœŸæ€§ä¿å­˜æ¨¡å‹ --------
        if i % cfg.save_interval == 0:
            ckpt_path = os.path.join(run.dir, f"checkpoint_{i}.pt")
            torch.save(policy.state_dict(), ckpt_path)
            print("[NavRL]: model saved at training step: ", i)

    # ============================================
    # ç¬¬ 10 æ­¥ï¼šè®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹
    # ============================================
    ckpt_path = os.path.join(run.dir, "checkpoint_final.pt")
    torch.save(policy.state_dict(), ckpt_path)
    print(f"[NavRL]: Training complete! Final model saved to {ckpt_path}")
    
    # å…³é—­ WandB å’Œä»¿çœŸå™¨
    wandb.finish()
    sim_app.close()

if __name__ == "__main__":
    main()
    