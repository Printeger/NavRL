Layer1 详细实现路线图
一、总体时间估计
保守估计：10-12周（2.5-3个月）

激进估计：8周（如果一切顺利）

我的建议：按12周规划，争取10周完成

关键时间分配：

基础设施搭建：2周
纯PPO基线：2周
Constrained PPO：2周
Domain Randomization + 鲁棒性：2周
真机准备与验证：3周
论文撰写：1周（与真机验证并行）
二、Phase 1：基础设施搭建（Week 1-2）
Week 1：仿真环境选型与搭建
任务1.1：仿真器选择
主要选项对比：

仿真器	优势	劣势	推荐度
Isaac Gym	超快（GPU并行），免费，5000+env	PyTorch绑定，学习曲线陡，碰撞检测简单	⭐⭐⭐⭐⭐
PyBullet	简单易用，物理精确，文档丰富	慢（单核CPU），难扩展到1000+env	⭐⭐⭐
Gazebo	真实感强，ROS集成好	极慢，不适合大规模RL	⭐⭐
AirSim	视觉真实，微软支持	重度依赖UE4，RL支持一般	⭐⭐
我的强烈建议：Isaac Gym

理由：

Layer1需要大量随机场景（10,000+），必须GPU并行
你的任务主要靠LiDAR，不需要逼真视觉
训练速度差异巨大：Isaac Gym 10M steps = 2-3小时，PyBullet = 2-3天
注意事项：

⚠️ Isaac Gym在Ubuntu上更稳定（Windows支持一般）
⚠️ 需要NVIDIA GPU（RTX 3060以上推荐）
⚠️ 碰撞检测相对简单（凸包近似），但对你的任务够用
备选方案：

如果Isaac Gym配置失败（>3天搞不定），退回PyBullet
可以PyBullet先做原型（100 envs），验证可行后迁移Isaac Gym
任务1.2：建立四旋翼动力学模型
需要决定的关键问题：

问题A：使用多高保真度的动力学？

保真度	包含内容	优势	劣势
低保真	简单质点+阻力	训练快，代码简单	Sim2Real Gap大
中保真	螺旋桨拉力+陀螺效应	平衡训练速度与真实性	需要调参
高保真	电机延迟+气动干扰	最真实	训练慢，难调试
我的建议：中保真 + Domain Randomization

具体包含：

✅ 基于螺旋桨转速的拉力/力矩模型
✅ 陀螺效应（重要！影响快速机动）
✅ 简单空气阻力（二次方拖拽）
❌ 不用建模：地面效应、旋翼干扰、复杂气动
为什么这样选？

太简单：网络学到的策略在真机上不work
太复杂：训练慢，且你后面会用Domain Randomization覆盖不确定性
注意事项：

⚠️ 务必从真机参数表获取：质量、转动惯量、电机时间常数
⚠️ 如果没有真机，先用开源飞控参数（PX4 Iris模型）
⚠️ 预留参数randomization接口（后面Phase 3会用）
任务1.3：实现3D LiDAR仿真
关键设计决策：

问题B：LiDAR模拟精度？

方案	实现	速度	真实度
简单射线投射	360个方向，返回最近距离	极快	中等
真实激光扫描	模拟真实LiDAR模式（如Velodyne VLP-16）	慢	高
预训练网络	从深度图生成点云	中等	高
我的建议：简单射线投射 + 噪声模型

具体实现：

apache
每一帧：
1. 在水平面360°均匀采样（如10°间隔 = 36射线）
2. 垂直方向3-5层（模拟多线LiDAR）
   总共：36 × 3 = 108维（可以调整密度）
3. 对每条射线：
   - 物理引擎ray-casting
   - 返回距离d
   - 添加高斯噪声：d' = d + N(0, 0.01) + 0.02×d（距离相关噪声）
4. 超出量程设为max_range（如10m）

注意事项：

⚠️ Isaac Gym的ray-casting可能有bug（多线程竞争），建议先单env测试
⚠️ 射线数量影响训练速度：先用粗糙版本（72维），验证概念后再加密
⚠️ 记录真实LiDAR的噪声特性（后面真机时对比）
备选方案：

如果Isaac Gym的ray-casting太慢，用离散体素图（voxel grid）预计算
任务1.4：构建受限环境库
需要的环境类型：

环境类型	参数	数量	目的
窄走廊	宽度0.6-2.0m，长度10-30m	200+	学习侧向避障
方形房间	边长3-8m，随机障碍物	200+	学习自由空间导航
圆形隧道	半径0.5-1.5m，长度10-20m	100+	学习管道环境
迷宫	分支走廊，死角	100+	学习复杂空间
动态障碍	移动物体速度0.5-2m/s	50+	（可选）处理动态
生成策略：

程序化生成：每次reset随机生成新环境
关键参数随机化：
走廊宽度：uniform(0.6, 2.0)
转角角度：[90°, 45°, T型]
障碍物位置/大小：随机
纹理/颜色：随机（如果用视觉）
注意事项：

⚠️ 确保环境有解（能通过），可以用A*验证可达性
⚠️ 记录环境复杂度标签（简单/中等/困难），便于curriculum learning
⚠️ 第一周先建10个手工环境验证，第二周再做程序化生成
备选方案：

如果程序化生成太复杂，先用50个手工设计环境
可以参考Habitat/Gibson数据集的建筑布局
Week 2：RL框架搭建与疯狂指令生成器
任务2.1：RL训练框架选择
主要选项：

框架	优势	劣势	推荐度
Stable-Baselines3	易用，文档好，社区大	不支持GPU并行env	⭐⭐⭐
RLlib (Ray)	分布式强，支持多种算法	配置复杂，debug难	⭐⭐⭐
CleanRL	代码简洁，易修改	需要自己实现很多东西	⭐⭐⭐⭐
rl_games	Isaac Gym官方推荐，GPU加速	文档少，社区小	⭐⭐⭐⭐⭐
我的建议：rl_games（配合Isaac Gym）

理由：

官方集成，天然支持GPU并行
训练速度最快（比SB3快10-50倍）
有Isaac Gym的官方example可以参考
注意事项：

⚠️ rl_games的API文档很差，建议先跑通官方示例（Ant, Cartpole等）
⚠️ 保留迁移到SB3的可能（后期真机部署时可能更方便）
⚠️ 建立良好的logging系统（TensorBoard + WandB）
备选方案：

如果rl_games学习成本太高（>3天），用CleanRL自己实现PPO
后期可以写adapter兼容多个框架
任务2.2：实现疯狂指令生成器
这是Layer1的核心创新之一，需要精心设计

指令类型分布（我的推荐）：

指令类型	占比	生成逻辑	目的
正常导航	30%	随机游走，避开障碍	基础能力
激进飞行	25%	高速（3-5m/s）+ 快速转向	高速避障
故意危险	20%	朝墙飞，撞向障碍	学习紧急修正
快速震荡	15%	高频正弦波指令	学习稳定性
静止悬停	10%	零速度指令	学习保持安全距离
具体实现细节：

正常导航指令：

每N步（如N=50）重新采样目标：
- 如果距离障碍>1.5m：随机速度0-2m/s
- 如果距离障碍<1.5m：采样远离障碍的方向
- 偶尔加入随机yaw旋转

激进飞行指令：

采样高速度：uniform(3, 5) m/s
方向：随机或沿走廊方向
持续时间：2-5秒
然后突然转向90-180°

故意危险指令：

检测最近障碍物方向
速度指向该方向，大小1-3m/s
持续1-3秒
→ 迫使网络学会拒绝危险指令

快速震荡指令：

vx = A × sin(2πft), A=2m/s, f=1-3Hz
vy = B × cos(2πft), B=1.5m/s
→ 测试网络的稳定性

注意事项：

⚠️ 不要一开始就用100%疯狂指令，使用curriculum：
Week 1-2: 80%正常 + 20%激进
Week 3-4: 50%正常 + 50%疯狂
Week 5+: 30%正常 + 70%疯狂
⚠️ 记录每条指令的"危险度"标签，便于后续分析
⚠️ 确保有足够的"可恢复"场景（不是每次都必然碰撞）
验证方法：

可视化100条随机指令轨迹，人工检查是否足够diverse
统计碰撞率：纯指令执行应该有30-50%碰撞率
任务2.3：定义观测和动作空间
观测空间（总维度：373维）：

组件	维度	具体内容	归一化方法
LiDAR	108	36方向×3层	除以max_range(10m)
IMU速度	3	[vx, vy, vz]	除以max_vel(5m/s)
IMU加速度	3	[ax, ay, az]	除以9.8m/s²
IMU角速度	3	[wx, wy, wz]	除以max_w(3rad/s)
人类指令	4	[cmd_vx, cmd_vy, cmd_vz, cmd_yaw]	除以对应max值
（可选）历史观测	3×3=9	过去3帧速度	帮助预测
动作空间：残差修正

方案	动作	优势	劣势
方案A	Δv = [Δvx, Δvy, Δvz, Δyaw]	直接，物理意义明确	需要限制幅度
方案B	缩放因子α ∈ [0,1] + 方向修正	更安全（不会反向）	复杂，难学习
我的建议：方案A（残差修正）

json
动作定义：
- 网络输出：tanh激活 → [-1, 1]⁴
- 实际修正：Δv = action × max_correction
  其中：max_correction = [1.5, 1.5, 1.0, 1.0] m/s 或 rad/s
- 最终指令：Safe_Cmd = Human_Cmd + Δv
- Clip到安全范围：[-5, 5] m/s

注意事项：

⚠️ max_correction是超参数，需要调优：
太大：网络可以完全忽略人类指令（失去残差意义）
太小：无法处理极端情况
建议初始值：1.5m/s（30%的最大速度5m/s）
⚠️ 考虑添加"置信度输出"（可选）：网络额外输出修正的必要性
⚠️ 记录修正幅度分布，验证网络是否真的在做"小幅修正"
验证方法：

训练后统计：95%的修正应该<max_correction的50%
如果经常饱和，说明max_correction设置太小
任务2.4：设计初步奖励函数
这是最关键的任务，决定了网络学到什么

奖励函数V1（基线版本）：

apache
总奖励 = w1×R_tracking + w2×R_safety + w3×R_smoothness + w4×R_residual

其中：
1. R_tracking = -||actual_vel - human_cmd||²
   目标：尽量跟随人类指令
   
2. R_safety:
   - 碰撞：-100（终止episode）
   - d<0.3m：-10
   - d<0.5m：-5
   - d<1.0m：-1
   - d>1.0m：0
   其中d = min(LiDAR readings)
   
3. R_smoothness = -||Δv_t - Δv_{t-1}||²
   目标：修正不要抖动
   
4. R_residual = -||Δv||
   目标：鼓励小幅修正

权重设置（初始版本）：

w1 = 1.0（基准）
w2 = 10.0（安全很重要）
w3 = 0.1（平滑次要）
w4 = 0.05（弱惩罚）

注意事项：

⚠️ 这只是起点，后面必然要调整（可能改10+次）
⚠️ 权重比例比绝对值更重要
⚠️ 建立自动化权重搜索：记录不同权重下的成功率
备选方案（Week 3-4会尝试）：

自适应权重：根据危险程度动态调整w1
奖励整形（Reward Shaping）：添加中间奖励（如"远离障碍物"）
课程学习奖励：早期重tracking，后期重safety
Week 2结束验证标准：
✅ 能在Isaac Gym中运行512个并行环境
✅ 疯狂指令生成器工作，可视化轨迹合理
✅ LiDAR可视化正确，碰撞检测准确
✅ PPO训练循环能跑通（即使策略很差）

三、Phase 2：纯PPO基线（Week 3-4）
Week 3：第一次训练
任务3.1：训练纯PPO（无约束）
训练配置：

apache
环境：512并行（根据GPU显存调整）
总步数：5-10M steps（约2-5小时）
学习率：3e-4（Adam）
GAE-λ：0.95
Discount γ：0.99
PPO epochs：10
Minibatch：4096

网络架构：

层	参数	说明
输入	373维	观测
Hidden 1	256，ReLU	
Hidden 2	256，ReLU	
Hidden 3	128，ReLU	
Actor输出	4，tanh	残差修正
Critic输出	1	V(s)
注意事项：

⚠️ 第一次训练一定会失败很多次，这很正常！
⚠️ 常见问题：
奖励不上升：检查奖励设计，可能太稀疏
策略崩溃（NaN）：降低学习率，检查observation归一化
碰撞率100%：可能环境太难，先用简单环境
⚠️ 优先修bug，而不是调超参数
训练监控指标：

asciidoc
核心指标（每1000步记录）：
- 平均回报（Episode Reward）
- 成功率（无碰撞完成的比例）
- 碰撞率
- 平均修正幅度（||Δv||）
- 指令跟踪误差
- 最小LiDAR距离分布

次要指标：
- Value loss
- Policy loss
- Entropy（策略探索性）
- KL divergence（策略更新幅度）

Debug技巧：

可视化第一个env：实时渲染，看网络在做什么
录制失败案例：保存导致碰撞的obs/action
检查数据分布：观测是否归一化正确，动作是否饱和
任务3.2：第一轮分析与改进
训练结束后必须回答的问题：

网络学到了什么？

测试：给固定指令（如"前进2m/s"），网络在不同环境下如何修正？
期望：在开阔区域几乎不修正，接近障碍时减速
失败模式是什么？

统计碰撞原因：
高速撞墙（没学会刹车）
卡在角落（局部最优）
侧面刮蹭（LiDAR盲区？）
奖励函数合理吗？

绘制奖励各分量的分布
检查是否有分量dominate（如safety太大，tracking被忽略）
可能需要的改进（Week 4）：

问题	可能原因	解决方案
学习慢	奖励稀疏	添加potential-based shaping
频繁碰撞	安全惩罚不够	增大w2，或添加软约束
过度保守	安全惩罚过大	减小w2，添加progress奖励
修正幅度大	residual惩罚太弱	增大w4，或减小max_correction
震荡抖动	smoothness惩罚太弱	增大w3，或添加加速度惩罚
Week 4：迭代优化PPO基线
任务4.1：超参数搜索
需要调的关键超参数（优先级排序）：

奖励权重 [w1, w2, w3, w4] —— 最重要
max_correction —— 影响修正能力
学习率 —— 影响训练稳定性
网络大小 —— 影响表达能力
搜索策略：

不要一次改多个！每次只改1-2个参数
每个配置至少训练3次（不同随机种子），取平均
记录在表格中：
配置ID	w1	w2	w3	w4	成功率	修正幅度	备注
baseline	1.0	10	0.1	0.05	45%	0.8m/s	碰撞多
config1	1.0	20	0.1	0.05	65%	0.6m/s	更保守
...							
注意事项：

⚠️ 不要过早放弃一个配置（至少2M steps）
⚠️ 成功率不是唯一指标，也看修正幅度和跟踪误差
⚠️ 建立自动化评估脚本（1000个测试场景）
任务4.2：添加Curriculum Learning（可选但推荐）
三阶段课程：

阶段	训练步数	环境难度	指令难度
Stage 1	0-2M	简单（宽走廊>1.5m）	80%正常
Stage 2	2M-5M	中等（走廊0.8-1.5m）	50%正常
Stage 3	5M+	困难（所有环境）	30%正常
实现方式：

根据当前训练步数，动态调整：
- 环境采样分布
- 指令生成器的分布参数

注意事项：

⚠️ 过渡要平滑，不要突然切换
⚠️ 可以根据成功率自适应：如果成功率>80%，提前进入下一阶段
⚠️ 记录每个阶段的性能，对比有无curriculum的差异
任务4.3：建立评估Benchmark
标准测试集（固定种子）：

简单场景（100个）：宽走廊，基本避障
复杂场景（100个）：窄走廊+转角
压力测试（50个）：极窄空间（0.6m）
动态障碍（50个）：移动物体
评估指标：

核心：
- 成功率（无碰撞完成）
- 平均修正幅度
- 指令跟踪误差（RMSE）

安全性：
- 碰撞率
- 平均最小距离
- 危险接近次数（d<0.3m）

效率：
- 完成时间
- 路径长度

Week 4结束标准：
✅ 纯PPO成功率 > 60%（简单场景）
✅ 平均修正幅度 < 1.0 m/s
✅ 有完整的评估benchmark
✅ 理解了奖励函数每个分量的作用

四、Phase 3：Constrained PPO（Week 5-6）
Week 5：实现约束
任务5.1：选择Constrained RL算法
主要选项：

算法	原理	实现难度	推荐度
PPO-Lagrangian	拉格朗日乘子法	中等	⭐⭐⭐⭐⭐
CPO	约束策略优化	高（需要二阶导）	⭐⭐⭐
TRPO-C	信赖域约束	高	⭐⭐
Safe Layer	后处理动作	简单	⭐⭐⭐
我的建议：PPO-Lagrangian

核心思想：

json
原始目标：max E[R]  s.t.  E[Cost] ≤ threshold

拉格朗日形式：
max E[R - λ×Cost]

其中λ（拉格朗日乘子）动态更新：
- 如果E[Cost] > threshold：λ增大（加重惩罚）
- 如果E[Cost] < threshold：λ减小（放松惩罚）

具体实现细节：

定义Cost函数：
Cost = binary_collision + soft_danger

其中：
- binary_collision = 1（碰撞），0（否）
- soft_danger = max(0, threshold - min_distance)
  例如threshold=0.5m，如果min_distance=0.3m → cost=0.2

约束目标：
E[Cost_binary] ≤ 0.05  （允许5%碰撞率）
E[Cost_soft] ≤ 0.1      （期望危险程度<0.1）

λ更新规则：
λ_t+1 = λ_t + lr_lambda × (E[Cost] - threshold)
并且clip到 [0, 10]

注意事项：

⚠️ lr_lambda是新的超参数，建议0.01-0.1
⚠️ λ初始值可以设为1.0
⚠️ 监控λ的变化：如果一直增长，说明约束难以满足
任务5.2：对比实验设计
必须对比的基线：

方法	说明	期望结果
PPO（无约束）	Week 4的最佳版本	成功率中等，可能有碰撞
PPO + 大safety权重	手工调w2=50	保守，成功率高但跟踪差
PPO-Lagrangian	你的方法	平衡安全与跟踪
（可选）Safe Layer	后处理过滤危险动作	简单但可能保守
实验协议：

每个方法训练5次（不同随机种子）
使用相同的评估benchmark
记录学习曲线（成功率vs训练步数）
关键对比指标：

1. 安全性：碰撞率、危险接近次数
2. 性能：指令跟踪RMSE、修正幅度
3. 效率：训练时间、样本效率
4. 稳定性：5次运行的方差

任务5.3：分析约束效果
需要回答的问题：

约束是否被满足？

绘制训练过程中的Cost曲线
检查是否收敛到threshold以下
λ如何演化？

绘制λ_t vs t
期望：初期快速增长，后期稳定
相比无约束PPO的改进？

统计显著性检验（t-test）
可视化典型成功/失败案例对比
可能遇到的问题：

现象	原因	解决方案
λ一直增长到上限	约束太严格或Cost定义不合理	放宽threshold或重新定义Cost
Cost忽高忽低不稳定	lr_lambda太大	降低学习率，或用移动平均
性能不如手工调权重	可能PPO-Lag不适合你的问题	回退到精细调参的PPO
Week 6：Domain Randomization
任务6.1：实现参数随机化
需要随机化的参数（优先级排序）：

参数	范围	重要性	理由
质量	±20%	⭐⭐⭐⭐⭐	真机质量不确定（电池电量影响）
转动惯量	±15%	⭐⭐⭐⭐	机械公差
LiDAR噪声	σ=1-5cm	⭐⭐⭐⭐⭐	传感器特性
控制延迟	10-50ms	⭐⭐⭐⭐	真机通信延迟
空气阻力系数	±30%	⭐⭐⭐	环境差异
电机时间常数	±20%	⭐⭐⭐	硬件差异
重心偏移	±2cm	⭐⭐	安装误差
外部风扰	0-1m/s	⭐⭐⭐	真实环境
实现方式：

每次env.reset()：
1. 采样新的随机参数
2. 更新仿真器物理参数
3. 重置观测噪声模型

注意事项：

⚠️ 不要一次加入所有随机化，分阶段：
Week 6.1: 质量 + 转动惯量
Week 6.2: LiDAR噪声 + 控制延迟
Week 6.3: 其余参数
⚠️ 每加一类随机化，重新评估性能
⚠️ 记录每个env的随机参数，便于分析
任务6.2：评估泛化性能
测试协议：

Seen Distribution（训练分布）：

用训练时的随机化参数测试
期望成功率：与训练末期相当
Unseen Distribution（域外泛化）：

极端参数：质量+30%，延迟100ms
新环境：训练时没见过的布局
期望成功率：>70%（轻度下降可接受）
Ablation Study（消融实验）：

只用质量随机化
只用噪声随机化
全部随机化
分析各自贡献
关键指标：

- 泛化Gap = 训练分布成功率 - 测试分布成功率
- 期望：Gap < 15%

任务6.3：Curriculum Domain Randomization（可选）
动态调整随机化强度：

训练阶段	随机化强度	理由
0-2M steps	低（±5%质量，1cm噪声）	让网络先学基础
2M-5M steps	中（±15%，3cm噪声）	逐步增加难度
5M+ steps	高（±20%，5cm噪声）	最终鲁棒性
实现：

根据训练进度或成功率自适应：
if success_rate > 80%:
    increase_randomization()

Week 6结束标准：
✅ Constrained PPO成功率 > 80%（训练分布）
✅ 泛化成功率 > 70%（测试分布）
✅ 碰撞率 < 5%（满足约束）
✅ 有消融实验证明各组件的作用

五、Phase 4：真机准备（Week 7-9）
Week 7：硬件集成
任务7.1：硬件清单确认
必需硬件：

组件	推荐型号	价格	备注
飞控	PX4 Pixhawk 6C	¥800	开源，社区大
机架	450mm四旋翼	¥500	留足空间装LiDAR
3D LiDAR	Livox Mid-360	¥3000	性价比高，FOV好
IMU	（飞控自带）	-	PX4内置6轴
机载计算机	Jetson Orin Nano	¥2000	或Raspberry Pi 4B（便宜）
电池	4S 5000mAh	¥300	续航15-20分钟
数传	（可选）5.8G图传	¥500	调试用
备选方案：

预算有限：用Livox MID-70（¥1500）或RoboSense RS-LiDAR-16
不想造飞机：买DJI F450 ARF套件（¥2000含机架+电机+电调）
注意事项：

⚠️ 确保LiDAR重量<300g（影响飞行性能）
⚠️ 检查机载计算机能否运行你的网络（推理速度>20Hz）
⚠️ 购买2套备件（电机、螺旋桨，必然会炸）
任务7.2：仿真到真机的Sim2Real清单
需要对齐的物理参数：

参数	获取方法	重要性
质量	称重（精确到10g）	⭐⭐⭐⭐⭐
转动惯量	测量or CAD计算	⭐⭐⭐⭐
电机推力系数	推力台测试	⭐⭐⭐⭐
LiDAR噪声	对墙扫描100次取方差	⭐⭐⭐⭐⭐
控制延迟	Timestamped logging	⭐⭐⭐⭐
LiDAR FOV/密度	数据手册	⭐⭐⭐⭐⭐
对齐步骤：

测量真机所有参数
更新仿真器配置文件
重新训练（使用真实参数作为DR的中心）
在仿真中回放真机飞行数据，检查是否匹配
注意事项：

⚠️ 仿真LiDAR的扫描模式必须和真实LiDAR一致
Livox是非重复性扫描（vs Velodyne的旋转扫描）
这会影响点云分布
⚠️ 飞控的PID参数会影响响应特性，记录下来
⚠️ 真机电池电压会影响性能，仿真中也要建模
任务7.3：搭建ROS2接口
软件架构：

真机：
PX4飞控 ←→ MAVROS ←→ ROS2 ←→ 你的节点（NN推理）
                            ↑
                      Livox ROS2 Driver

需要实现的ROS2节点：

节点名	功能	输入	输出
lidar_processor	处理点云	/livox/lidar	/processed_lidar（108维）
imu_processor	IMU数据	/mavros/imu	/processed_imu（9维）
policy_node	NN推理	obs	safe_cmd
safety_monitor	紧急保护	LiDAR	触发降落
关键代码模块：

模型加载：

使用ONNX或TorchScript导出（不要用.pth，不稳定）
测试推理延迟：<10ms
观测预处理：

点云转换为108维LiDAR观测（和训练时完全一致）
归一化：和训练时用同一个scaler
动作后处理：

网络输出 → 修正 → 叠加人类指令 → 发送给PX4
注意事项：

⚠️ 时间同步极其重要：LiDAR和IMU必须时间戳对齐
⚠️ 测试整个pipeline的延迟：期望<50ms（obs到cmd）
⚠️ 实现软件安全开关：如果推理超时，立即悬停
Week 8-9：真机实验
任务8.1：分阶段测试（Curriculum for Real）
绝对不要直接飞复杂环境！

阶段	环境	人类指令	期望	失败处理
Stage 0	室内开阔空间（5m×5m无障碍）	简单悬停	稳定悬停1分钟	手动接管
Stage 1	单个障碍物（如柱子）	慢速绕行（0.5m/s）	保持距离>0.5m	降落
Stage 2	简易走廊（2m宽）	沿走廊飞行（1m/s）	不碰撞飞过10m	紧急停止
Stage 3	窄走廊（1m宽）	中速（2m/s）	成功率>80%	分析失败
Stage 4	复杂环境（转角+障碍）	疯狂指令测试	展示鲁棒性	修正策略
每个阶段至少10次飞行，成功率>90%才进入下一阶段

注意事项：

⚠️ 安全第一：
所有测试必须有安全员随时准备遥控接管
飞行高度<2m（防止摔坏）
围网/软垫保护
⚠️ 详细记录每次飞行：
录像（第三人称视角）
ROS bag（所有传感器数据）
成功/失败原因
⚠️ 准备炸机（预算留10%买备件）
任务8.2：Sim2Real Gap分析
如果真机表现不好（成功率<60%），系统性排查：

可能原因	检查方法	解决方案
LiDAR观测不匹配	对比仿真和真实点云	调整仿真LiDAR模式
延迟太大	测量端到端延迟	优化代码，或重训练加入延迟
动力学差异	回放数据，对比轨迹	重新标定参数，重训
噪声模型不准	分析传感器噪声谱	调整DR噪声参数
网络过拟合	测试泛化性能	增加DR强度
PID参数影响	对比期望vs实际速度	调整PX4 PID
DAgger迭代（如果Sim2Real Gap>20%）：

Round 1:
1. 用仿真训练的策略在真机收集100条轨迹
2. 记录人类安全员的接管指令（作为expert）
3. 混合真机数据和仿真数据重训（比例1:10）
4. 测试新策略

Round 2-3: 重复上述过程

注意事项：

⚠️ 真机数据很宝贵，确保数据质量（标注准确）
⚠️ DAgger可能需要2-3轮迭代
⚠️ 如果3轮后仍不好，可能是仿真环境根本性问题
任务8.3：压力测试与Demo录制
测试项目：

极限速度测试：

指令：3m/s高速
环境：1.2m走廊
目标：不碰撞飞行20m
疯狂指令测试：

随机指令生成器（和训练时相同）
50次飞行，统计碰撞率
目标：<10%
长时间飞行：

连续飞行5分钟
目标：稳定无崩溃
对比实验（录制视频）：

无Layer1（直接执行人类指令）→ 碰撞
有Layer1 → 成功
并排对比展示价值
Demo录制清单：

✅ 慢动作特写（网络如何修正危险指令）
✅ 多角度摄像（展示环境复杂性）
✅ 屏幕录制（可视化LiDAR + 网络输出）
✅ 失败案例（诚实展示局限性）
Week 9结束标准：
✅ 真机成功率 > 75%（中等难度环境）
✅ Sim2Real Gap < 20%
✅ 有压力测试视频证明鲁棒性
✅ 理解了真机与仿真的主要差异

六、Phase 5：论文撰写（Week 10-12）
Week 10-11：实验与分析
任务10.1：补充实验
必须有的对比实验：

实验名称	对比内容	预期结论
Baseline对比	PPO vs PPO-Lag vs MPC	证明方法优势
Ablation	w/o DR, w/o Curriculum, w/o Residual	证明各组件必要性
泛化测试	训练vs测试环境	证明鲁棒性
真机验证	仿真vs真机性能	证明实用性
极限测试	不同速度/环境难度	探索能力边界
任务10.2：结果可视化
必须有的图表：

学习曲线：成功率 vs 训练步数（多方法对比）
成功率分布：不同环境类型的box plot
修正幅度分析：直方图展示残差分布
真机轨迹：3D可视化飞行路径
失败案例分析：典型失败模式的截图+分析
任务10.3：撰写方法部分
论文结构建议（RAL格式，6页）：

markdown
Abstract
1. Introduction（1页）
   - 动机：为什么需要安全的指令过滤
   - 挑战：传统方法的局限（CBF等）
   - 贡献：Constrained RL + 残差设计 + 真机验证

2. Related Work（0.5页）
   - Safe RL
   - 四旋翼避障
   - Sim2Real

3. Method（2页）
   - 问题形式化
   - 网络架构（残差设计）
   - PPO-Lagrangian
   - Domain Randomization
   - 训练细节

4. Experiments（2页）
   - 仿真设置
   - 对比实验
   - Ablation
   - 真机结果

5. Conclusion（0.5页）
   - 总结
   - 局限性
   - 未来工作（Layer2）

注意事项：

⚠️ RAL要求简洁，删掉所有冗余
⚠️ Method部分要有数学形式化（即使很简单）
⚠️ 真机部分要详细描述硬件setup（可复现性）
Week 12：投稿准备
任务12.1：代码开源准备
必须包含：

✅ 训练代码（可运行的最小示例）
✅ 预训练模型（.onnx格式）
✅ 评估脚本
✅ 真机部署ROS2代码
✅ 详细README（环境配置+快速开始）
✅ 视频链接（YouTube + Bilibili）
代码仓库结构：

layer1_safe_cmd_filter/
├── sim/
│   ├── envs/          # Isaac Gym环境
│   ├── train.py       # 训练脚本
│   ├── eval.py        # 评估脚本
│   └── configs/       # 超参数配置
├── real_robot/
│   ├── ros2_ws/       # ROS2工作空间
│   └── deploy/        # 部署脚本
├── models/
│   └── pretrained/    # 预训练权重
├── docs/              # 文档
└── README.md

任务12.2：视频制作
视频结构（3-5分钟）：

开场（30s）：问题motivation，展示危险指令
方法（1min）：动画解释残差设计+Constrained RL
仿真结果（1min）：对比实验，展示学习过程
真机Demo（2min）：多种环境测试，包括失败案例
结尾（30s）：总结+未来工作
注意事项：

⚠️ 配字幕（英文）
⚠️ 节奏快，不要有冗长的静止画面
⚠️ 背景音乐不要太吵
七、风险预案与备选方案
高风险点识别
风险	概率	影响	缓解措施
Isaac Gym配置失败	30%	延迟1周	备选PyBullet，并行尝试
PPO训练不收敛	40%	延迟2周	降低任务难度，调整奖励
Sim2Real Gap太大	50%	延迟3周	DAgger迭代，增加DR
真机硬件故障	20%	延迟1周	买2套备件
论文被拒	30%	重投	投稿前内部审稿
时间缓冲策略
如果Week 6结束时进度<50%：

砍掉可选内容：
不做MPC baseline对比
减少ablation实验
简化真机环境
调整目标：
先投RAL Letter（4页简化版）
完整版后续投TRO
如果真机实验失败：

仍然可以发纯仿真论文（RAL接受仿真论文）
重点突出方法创新（Constrained RL + 残差设计）
在Discussion中诚实讨论Sim2Real挑战
八、每周检查点（Checkpoint）
建立每周五的自我检查清单：

Week 1 ✓
 Isaac Gym安装成功，能运行官方demo
 四旋翼动力学仿真可运行
 LiDAR射线投射正确，可视化验证
 至少有10个手工环境
Week 2 ✓
 疯狂指令生成器实现，可视化100条轨迹
 RL训练循环跑通（即使策略很差）
 观测和动作归一化正确
 TensorBoard能看到训练曲线
Week 3 ✓
 纯PPO训练完成（至少5M steps）
 成功率>40%（简单环境）
 理解了失败模式
 有初步的分析报告
Week 4 ✓
 完成超参数搜索（至少3组配置）
 成功率>60%
 建立了标准评估benchmark
 修正幅度<1.0m/s
Week 5 ✓
 PPO-Lagrangian实现并验证
 约束被满足（碰撞率<5%）
 对比实验完成
Week 6 ✓
 Domain Randomization实现
 泛化Gap<20%
 成功率>80%（训练分布）
Week 7 ✓
 硬件到位并组装完成
 仿真参数与真机对齐
 ROS2接口跑通
Week 8 ✓
 Stage 0-1测试通过
 首次真机飞行成功（无碰撞）
Week 9 ✓
 Stage 2-3测试通过
 真机成功率>70%
 Demo视频录制完成
Week 10-11 ✓
 所有对比实验完成
 论文初稿完成
Week 12 ✓
 代码开源准备完毕
 论文提交
九、资源需求清单
计算资源
GPU：RTX 3070及以上（8GB显存最低）
CPU：8核以上
内存：32GB推荐
存储：100GB（数据+模型）
时间投入
全职：10-12周
兼职（50%时间）：20-24周
预算
项目	金额
真机硬件	¥7000
备件	¥1000
测试场地租赁（可选）	¥2000
总计	¥10,000
十、核心建议与成功关键
我的3个最重要建议：
小步快跑，快速验证

不要憋大招，Week 2就要看到第一个能飞的agent
即使很笨，也比纸上谈兵强
详细记录一切

每天写实验日志（哪怕只有3行）
记录所有超参数配置和结果
3个月后你会忘记为什么这样设计
预留20%缓冲时间

肯定会有意外（代码bug、硬件故障）
不要把计划排满
可能让你卡住的3个坑：
奖励函数调不对

解决：先用最简单的奖励（只有碰撞惩罚），能work后再复杂化
Sim2Real Gap太大

解决：不要期望100%迁移，70%就很好了，剩下的DAgger补
真机调试效率低

解决：大部分工作在仿真做完，真机只做最终验证